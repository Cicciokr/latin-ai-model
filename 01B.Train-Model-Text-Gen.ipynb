{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10696023,
          "sourceType": "datasetVersion",
          "datasetId": 6471323
        }
      ],
      "dockerImageVersionId": 30888,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y tensorflow && pip install tensorflow-cpu\n",
        "!pip install datasets\n",
        "#!pip install -U optax jax[tpu]==0.4.30 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from time import time\n",
        "import random\n",
        "\n",
        "\n",
        "def sentence_permutation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    A document is divided into sentences based on full stops, and these sentences are shuffled in a random order.\n",
        "    **This function operates on text strings.**\n",
        "    :param sentence: The sentence to be permuted.\n",
        "    :return: The permuted sentence.\n",
        "    \"\"\"\n",
        "    sentences = text.split(\".\")\n",
        "    permuted_sentences = torch.randperm(len(sentences))\n",
        "    permuted_text = \"\"\n",
        "    for i in permuted_sentences:\n",
        "        if sentences[i] != \"\":\n",
        "            permuted_text += sentences[i] + \". \"\n",
        "    return permuted_text.strip()\n",
        "\n",
        "\n",
        "def token_infilling(\n",
        "    tokenized_sequence: torch.Tensor,\n",
        "    mask_token_id: int,\n",
        "    mask_probability: float = 0.15,\n",
        "    list_special_tokens: list = [],\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    A number of text spans are sampled, with span lengths drawn from a Poisson distribution (Î» = 3).\n",
        "    Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of\n",
        "    [MASK] tokens. Text infilling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples\n",
        "    span lengths from a different (clamped geometric) distribution, and replaces each span with a\n",
        "    sequence of [MASK] tokens of exactly the same length. Text infilling teaches the model to predict\n",
        "    how many tokens are missing from a span.\n",
        "    **This function operates on tokenized text.**\n",
        "    :param text: The text to be infilled.\n",
        "    :return: The infilled text.\n",
        "    \"\"\"\n",
        "    span_length = int(torch.poisson(torch.tensor([3.0])))\n",
        "    perturbed_ids = torch.empty(0, dtype=torch.long)\n",
        "    if span_length > 0:\n",
        "        for i in range(0, len(tokenized_sequence), span_length):\n",
        "            if torch.rand(1) < mask_probability:\n",
        "                # check if the span does not contain special tokens\n",
        "                if not any(token in list_special_tokens for token in tokenized_sequence[i : i + span_length]):\n",
        "                    perturbed_ids = torch.cat(\n",
        "                        (perturbed_ids, torch.tensor([mask_token_id], dtype=torch.long))\n",
        "                    )\n",
        "            else:\n",
        "                perturbed_ids = torch.cat(\n",
        "                    (perturbed_ids, tokenized_sequence[i : i + span_length])\n",
        "                )\n",
        "    else:\n",
        "        perturbed_ids = tokenized_sequence # if the span length is 0, the text is not perturbed\n",
        "    return perturbed_ids\n",
        "\n",
        "\n",
        "def token_masking(\n",
        "    tokenized_sequence: torch.Tensor,\n",
        "    mask_token_id: int,\n",
        "    mask_probability: float = 0.15,\n",
        "    list_special_tokens: list = [],\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Random tokens are replaced with the [MASK] token. This task trains the model to predict the original value of the masked tokens.\n",
        "    **This function operates on tokenized text.**\n",
        "    :param text: The text to be masked.\n",
        "    :return: The masked text.\n",
        "    \"\"\"\n",
        "    for i in range(len(tokenized_sequence)):\n",
        "        if torch.rand(1) < mask_probability:\n",
        "            if tokenized_sequence[i] not in list_special_tokens:\n",
        "                tokenized_sequence[i] = mask_token_id\n",
        "    return tokenized_sequence\n",
        "\n",
        "\n",
        "def token_deletion(\n",
        "    tokenized_sequence: torch.Tensor,\n",
        "    mask_token_id: int,\n",
        "    mask_probability: float = 0.15,\n",
        "    list_special_tokens: list = [],\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Random tokens are deleted from the input. In contrast to token masking, the model must decide which positions are missing inputs.\n",
        "    **This function operates on tokenized text.**\n",
        "    :param text: The text to be token deleted.\n",
        "    :return: The token deleted text.\n",
        "    \"\"\"\n",
        "    delete_mask = torch.rand(len(tokenized_sequence)) < mask_probability\n",
        "    tokenized_sequence = tokenized_sequence[~delete_mask]\n",
        "    return tokenized_sequence\n",
        "\n",
        "\n",
        "def document_rotation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n",
        "    This task trains the model to identify the start of the document.\n",
        "    **This function operates on text strings.**\n",
        "    :param text: The text to be rotated.\n",
        "    :return: The rotated text.\n",
        "    \"\"\"\n",
        "    text = text.split(\" \")\n",
        "    rotation_index = random.randint(0, len(text) - 1)\n",
        "    rotated_text = text[rotation_index:] + text[:rotation_index]\n",
        "    return \" \".join(rotated_text)\n",
        "\n",
        "\n",
        "# PARAMETERS BART BASE\n",
        "# ==============================================================================\n",
        "VOCAB_SIZE = 52000\n",
        "MAX_POSITION_EMBEDDINGS = 1024\n",
        "ENCODER_LAYERS = 6\n",
        "ENCODER_FFN_DIM = 3072\n",
        "ENCODER_ATTENTION_HEADS = 12\n",
        "DECODER_LAYERS = 6\n",
        "DECODER_FFN_DIM = 3072\n",
        "DECODER_ATTENTION_HEADS = 12\n",
        "D_MODEL = 768\n",
        "DROPOUT = 0.1\n",
        "# ==============================================================================\n",
        "# PARAMETERS\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a BART-Base model\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/kaggle/input/the-latin-library/tokenizer_bart_la\")\n",
        "\n",
        "\n",
        "# Tiny version of BART\n",
        "model = BartForConditionalGeneration(\n",
        "    BartConfig(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
        "        encoder_layers=ENCODER_LAYERS,\n",
        "        encoder_ffn_dim=ENCODER_FFN_DIM,\n",
        "        encoder_attention_heads=ENCODER_ATTENTION_HEADS,\n",
        "        decoder_layers=DECODER_LAYERS,\n",
        "        decoder_ffn_dim=DECODER_FFN_DIM,\n",
        "        decoder_attention_heads=DECODER_ATTENTION_HEADS,\n",
        "        d_model=D_MODEL,\n",
        "        dropout=DROPOUT,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        is_encoder_decoder=True,\n",
        "        decoder_start_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        ")\n",
        "\n",
        "dataset_latin = load_dataset(\"text\", split='train', data_files=\"/kaggle/input/the-latin-library/thelatinlibrary_cleaned.txt\").with_format(type=\"torch\")\n",
        "dataset_perseus = load_dataset(\"text\", split='train', data_files=\"/kaggle/input/the-latin-library/perseus_complete_cleaned.txt\").with_format(type=\"torch\")\n",
        "dataset = concatenate_datasets([dataset_latin, dataset_perseus])\n",
        "dataset_split = dataset.train_test_split(test_size=0.2, train_size=0.8, shuffle=True)\n",
        "print(dataset_split)\n",
        "train_streaming_dataset = dataset_split['train']\n",
        "eval_streaming_dataset = dataset_split['test']\n",
        "print(train_streaming_dataset)\n",
        "print(eval_streaming_dataset)\n",
        "\n",
        "# perturbation in string: document_rotation, sentence_permutation\n",
        "# perturbation in token : token_infilling, token_masking, token_deletion\n",
        "perturbations = [\n",
        "    document_rotation,\n",
        "    sentence_permutation,\n",
        "    token_infilling,\n",
        "    token_masking,\n",
        "    token_deletion,\n",
        "]\n",
        "\n",
        "perturbations_text_domain = [\n",
        "    document_rotation,\n",
        "    sentence_permutation,\n",
        "]\n",
        "\n",
        "perturbations_token_domain = [\n",
        "    token_infilling,\n",
        "    token_masking,\n",
        "    token_deletion,\n",
        "]\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"\n",
        "    Collate function to be used in the dataloader.\n",
        "    It applies the perturbations to the examples and returns the batch.\n",
        "    TODO: improve efficiency\n",
        "    :param examples: list of examples\n",
        "    :return: batch ready to be fed to the model\n",
        "    \"\"\"\n",
        "    original_texts = [example[\"text\"] for example in examples]\n",
        "\n",
        "    input_ids = None\n",
        "    for text in original_texts:\n",
        "        perturbation_function = random.choice(perturbations)\n",
        "        if perturbation_function in perturbations_text_domain:\n",
        "            # need to truncate the text to 1024 tokens\n",
        "            t_text = tokenizer(text, truncation=True, max_length=1024)\n",
        "            text_truncated = tokenizer.decode(t_text[\"input_ids\"], skip_special_tokens=True)\n",
        "            perturbed_text = perturbation_function(text_truncated)\n",
        "            perturbed_input_ids = tokenizer(\n",
        "                perturbed_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_POSITION_EMBEDDINGS\n",
        "            )[\"input_ids\"][0]\n",
        "        else:\n",
        "            original_input_ids = tokenizer(\n",
        "                text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_POSITION_EMBEDDINGS\n",
        "            )[\"input_ids\"][0]\n",
        "            perturbed_input_ids = perturbation_function(\n",
        "                    tokenized_sequence=original_input_ids,\n",
        "                    mask_token_id=tokenizer.mask_token_id,\n",
        "                    mask_probability=0.15,\n",
        "                    list_special_tokens=tokenizer.all_special_ids,\n",
        "                )\n",
        "            if perturbed_input_ids.shape[-1] < MAX_POSITION_EMBEDDINGS: # apply padding\n",
        "                perturbed_input_ids = torch.cat(\n",
        "                    (perturbed_input_ids, torch.full((MAX_POSITION_EMBEDDINGS - perturbed_input_ids.shape[-1],),\n",
        "                    tokenizer.pad_token_id,\n",
        "                    dtype=torch.long)))\n",
        "            perturbed_input_ids = torch.squeeze(perturbed_input_ids, dim=0)\n",
        "\n",
        "        if input_ids is None:\n",
        "            input_ids = perturbed_input_ids.unsqueeze(0)\n",
        "        else:\n",
        "            input_ids = torch.cat((input_ids, perturbed_input_ids.unsqueeze(0)), dim=0)\n",
        "\n",
        "    tokenized_examples = {}\n",
        "    # update the tokenized examples with the perturbed input ids and convert to tensors\n",
        "    tokenized_examples[\"input_ids\"] = input_ids\n",
        "    # update the attention mask\n",
        "    tokenized_examples[\"attention_mask\"] = [\n",
        "        [1 if token_id != tokenizer.pad_token_id else 0 for token_id in input_ids]\n",
        "        for input_ids in tokenized_examples[\"input_ids\"]\n",
        "    ]\n",
        "    tokenized_examples[\"attention_mask\"] = torch.tensor(tokenized_examples[\"attention_mask\"])\n",
        "\n",
        "    tokenized_examples[\"labels\"] = tokenizer(\n",
        "        original_texts, padding=\"max_length\", truncation=True, max_length=MAX_POSITION_EMBEDDINGS, return_tensors=\"pt\"\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "# total_steps (1 epoch, see it5) = 103_000_000 / 64 = 1_609_375 -- 1_700_000\n",
        "# warmup_steps = 1_700_000 * 0.01 = 17_000\n",
        "\n",
        "# Prepare training arguments\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"/kaggle/working/bart-la-size-s\",\n",
        "    overwrite_output_dir=True,\n",
        "    auto_find_batch_size=True,\n",
        "    num_train_epochs=5,\n",
        "    warmup_steps=2000,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"/kaggle/working/logs-bart-it-size-s\",\n",
        "    logging_steps=300,\n",
        "    save_total_limit=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True,\n",
        "    #tpu_num_cores=8,\n",
        "    dataloader_num_workers=12,\n",
        "    learning_rate=1e-5,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_streaming_dataset,\n",
        "    eval_dataset=eval_streaming_dataset,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"train started\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "print(trainer.evaluate(eval_streaming_dataset))\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"/kaggle/working/bart-la-s\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T13:59:36.445671Z",
          "iopub.execute_input": "2025-02-08T13:59:36.446048Z",
          "iopub.status.idle": "2025-02-08T14:00:29.268360Z",
          "shell.execute_reply.started": "2025-02-08T13:59:36.446008Z",
          "shell.execute_reply": "2025-02-08T14:00:29.266955Z"
        },
        "id": "SduF7Pb_I2Hf"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}