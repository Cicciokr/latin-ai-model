{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODgZ7UqBYjKYf80Awd2HzW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cicciokr/latin-ai-model/blob/main/Fine_Tuning_Transformer_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn2rLr9Zui83"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
        "from transformers import RobertaTokenizerFast, RobertaConfig, RobertaForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "st.title(\"Training\")\n",
        "#print(torch.cuda.device_count())\n",
        "#print(torch.cuda.get_device_name(0))\n",
        "#dataset testo\n",
        "#dataset = load_dataset('text', data_files='la.txt')\n",
        "#dataset parquet\n",
        "#dataset = load_dataset(\"Cicciokr/CC-100-Latin\", revision=\"refs/convert/parquet\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = RobertaTokenizerFast(\n",
        "    vocab_file=\"./latinroberta-vocab.json\",\n",
        "    merges_file=\"./latinroberta-merges.txt\",\n",
        ")\n",
        "config = RobertaConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    max_position_embeddings=514,  # Lunghezza massima della sequenza\n",
        "    hidden_size=768,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1\n",
        ")\n",
        "print(tokenizer.mask_token)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "# Applicare la tokenizzazione\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "model.to(device)\n",
        "#dataset = load_dataset(\"parquet\", data_dir=\"./parquet\", trust_remote_code=True)\n",
        "#tokenized_dataset = dataset.map(preprocess_function, batched=True, num_proc=4)\n",
        "#tokenized_dataset.save_to_disk(\"./dataset/tokenized_dataset\")\n",
        "#tokenized_dataset = load_from_disk(\"./dataset_light/tokenized_dataset\")\n",
        "dataset = load_dataset(\"pstroe/cc100-latin\", data_files=\"la.nolorem.tok.latalphabetonly.v2.json\", field=\"train\")\n",
        "dataset_split = dataset['train'].train_test_split(test_size=0.0001, train_size=0.0003, shuffle=True)\n",
        "print(dataset_split)\n",
        "tokenized_datasets_test = dataset_split['test'].map(preprocess_function, batched=True, num_proc=4)\n",
        "tokenized_datasets_train = dataset_split['train'].map(preprocess_function, batched=True, num_proc=4)\n",
        "\n",
        "data_collator = DataCollatorForWholeWordMask(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,                   # Abilita il mascheramento\n",
        "    mlm_probability=0.15        # Percentuale di token da mascherare\n",
        ")\n",
        "\n",
        "#il 20% dei dati viene usato come test e l'80% viene usato come train, per evitare overfitting\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    metric_for_best_model=\"perplexity\",\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "#metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Calcola la perplexit√†\n",
        "    perplexity = math.exp(torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='mean'))\n",
        "    return {\"perplexity\": perplexity}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets_train,\n",
        "    eval_dataset=tokenized_datasets_test,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#dataset1 = tokenized_dataset['train'].select(range(0, 50000))\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"lat-roberta\")\n",
        "tokenizer.save_pretrained(\"lat-roberta\")\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    }
  ]
}