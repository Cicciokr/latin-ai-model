{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPddILmHpB1tjngo29ozIVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cicciokr/latin-ai-model/blob/main/Calculate_NPMI_TD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z0E6p6Dir-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "\n",
        "# Funzione per calcolare la coerenza dei topic (NPMI)\n",
        "def calculate_npmi(topics, documents, vocab):\n",
        "    vectorizer = CountVectorizer(vocabulary=vocab)\n",
        "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    def npmi_score(topic):\n",
        "        scores = []\n",
        "        for i in range(len(topic)):\n",
        "            for j in range(i + 1, len(topic)):\n",
        "                word_i = topic[i]\n",
        "                word_j = topic[j]\n",
        "                p_i = np.sum(doc_term_matrix[:, vocab.index(word_i)].toarray()) / len(documents)\n",
        "                p_j = np.sum(doc_term_matrix[:, vocab.index(word_j)].toarray()) / len(documents)\n",
        "                p_ij = np.sum(\n",
        "                    (doc_term_matrix[:, vocab.index(word_i)].toarray() > 0) &\n",
        "                    (doc_term_matrix[:, vocab.index(word_j)].toarray() > 0)\n",
        "                ) / len(documents)\n",
        "                if p_ij > 0:\n",
        "                    score = np.log(p_ij / (p_i * p_j)) / -np.log(p_ij)\n",
        "                else:\n",
        "                    score = 0\n",
        "                scores.append(score)\n",
        "        return np.mean(scores)\n",
        "\n",
        "    return np.mean([npmi_score(topic) for topic in topics])\n",
        "\n",
        "# Funzione per calcolare la diversità dei topic (TD)\n",
        "def calculate_td(topics):\n",
        "    unique_words = set()\n",
        "    total_words = 0\n",
        "    for topic in topics:\n",
        "        unique_words.update(topic)\n",
        "        total_words += len(topic)\n",
        "    return len(unique_words) / total_words\n",
        "\n",
        "# Funzione per generare embedding con RoBERTa\n",
        "def generate_embeddings(documents):\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "    embeddings = []\n",
        "    for doc in documents:\n",
        "        inputs = tokenizer(doc, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(1).detach().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Funzione per estrarre i topic con LDA\n",
        "def extract_topics(documents, n_topics=5, n_words=10):\n",
        "    vectorizer = CountVectorizer()\n",
        "    doc_term_matrix = vectorizer.fit_transform(documents)\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    lda.fit(doc_term_matrix)\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        topics.append([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_words - 1:-1]])\n",
        "    return topics\n",
        "\n",
        "# Funzione principale per calcolare i parametri\n",
        "def evaluate_topics(documents, topics):\n",
        "    # Prepara il vocabolario\n",
        "    vocab = list(set(word for topic in topics for word in topic))\n",
        "\n",
        "    # Calcola NPMI\n",
        "    npmi = calculate_npmi(topics, documents, vocab)\n",
        "\n",
        "    # Calcola TD\n",
        "    td = calculate_td(topics)\n",
        "\n",
        "    # Calcola le distanze con embedding RoBERTa\n",
        "    embeddings = generate_embeddings(documents)\n",
        "    distances = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "    return {'NPMI': npmi, 'TD': td, 'Embedding Distances': distances}\n",
        "\n",
        "# Esempio di utilizzo\n",
        "documents = [\n",
        "    \"Roma è la capitale dell'Impero Romano.\",\n",
        "    \"Cesare fu un grande condottiero romano.\",\n",
        "    \"La letteratura latina è ricca di poesia epica e drammatica.\"\n",
        "]\n",
        "\n",
        "# Estrai i topic\n",
        "topics = extract_topics(documents, n_topics=3, n_words=5)\n",
        "print(\"Topics estratti:\", topics)\n",
        "\n",
        "# Calcola metriche\n",
        "evaluation = evaluate_topics(documents, topics)\n",
        "print(evaluation)"
      ]
    }
  ]
}